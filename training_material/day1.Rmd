---
title: "Introduction to Bayesian Disease Measurement for Health Scientists - Day 1 | 2"
author:
- Eleftherios Meletis
- Polychronis Kostoulas
- Evangelia Anyfanti
date: '2024-04-22 & 2024-04-23'
output:
  slidy_presentation: default
  beamer_presentation: default
  ioslides_presentation: default
params:
  presentation: yes
subtitle: CA18208 HARMONY Larissa Training School 2024 - https://harmony-net.eu/
---

```{r rendering, eval=FALSE, include=FALSE}
# To render this as PDF (beamer) slides run:
rmarkdown::render('day1.Rmd', 'beamer_presentation', params=list(presentation=TRUE))
```

```{r setup, include=FALSE}
library("tidyverse")
library("runjags")
library("rjags")
runjags.options(silent.jags=TRUE, silent.runjags=TRUE)
set.seed(2024-04-22)

# Reduce the width of R code output for PDF only:
if(params$presentation){
  knitr::knit_hooks$set(size = function(before, options, envir) {
    if(before){
      knitr::asis_output(paste0("\\", options$size))
    }else{
      knitr::asis_output("\\normalsize")
    }
  })
  knitr::opts_chunk$set(size = "scriptsize")
}

# Collapse successive chunks:
space_collapse <- function(x){ gsub("```\n*```r*\n*", "", x) }
# Reduce space between chunks:
space_reduce <- function(x){ gsub("```\n+```\n", "", x) }
knitr::knit_hooks$set(document = space_collapse)


# To collect temporary filenames:
cleanup <- character(0)
```

# Bayesian Statistics

In this session we'll see how we can estimate a probability of interest  but in a Bayesian framework, i.e. using Bayes theorem.

Bayes' theorem  - P(A|B) = P(B|A)*P(A)/P(B)
 
Components

 * P(A|B): Prob of event A occurring given that B is true - Posterior probability
 * P(B|A): Prob of event B occurring given that A is true - Likelihood ~ function of A
 * P(A): Prob of event A occurring - Prior probability
 * P(B): Prob of event B occurring - Marginal probability ~ sum over all possible values of A

# What we usually see/use

* theta: parameter of interest | y: observed data}
* P(theta|y) = P(y|theta) * P(theta)/P(y) 

Where:

 * P(theta): Prior probability of parameter(s) of interest;
 * P(y|theta): Likelihood of the data given the parameters value(s) 
 * P(theta|y): Posterior probability of parameter(s) of interest given the data and the prior


# Bayesian Inference - Summary & Example

To estimate the posterior distribution P(theta|y) we need to:

 *  Specify the Prior distribution: P(theta)
 *  Define the Likelihood of the data: P(y|theta) 

 
# Example 1

Hemophilia is a disease that exhibits X-chromosome-linked recessive inheritance. Consider a woman who has an affected brother, which implies that her mother must be a carrier of the hemophilia gene. We are also told that her father is not affected. Suppose she has two sons, neither of whom is affected.

* The unknown quantity of interest is the state of the woman.

# Exercise 1: Bayesian apparent prevalence (ap) estimation

y out of n individuals test positive. Estimate the apparent prevalence in a Bayesian framework.

 * Parameter of interest: ap - [0,1]

 * Data: n tested, y positive

 * Prior distribution for ap: ap ~ Beta(a,b)
 * Likelihood: y ~ Binomial(n,ap) 

# Let's write our first JAGS model

```{r}
ap_model <- 
  'model {
  
  # Define likelihood distribution of the data
  # JAGS Binomial distribution Arguments: p, n 
  
  y1 ~ dbin(ap1,n1)
  y2 ~ dbin(ap2,n2)
  
  # Specify prior distribution for par of interest 
  # Uniform (non-informative) prior distribution 
  ap1 ~ dbeta(1, 100000)
  ap2 ~ dbeta(1, 100000)


  #data# n1, y1, n2, y2
  #monitor# ap1, ap2
  #inits# ap1
  }
  '
```

# Let's run our first JAGS model
```{r}
# Call JAGS
library(runjags)

# Provide Data 
n1 = 4000
y1 = 1200

n2 = 40
y2 = 12

```

#
```{r}
# Initial values for par of interest

ap1 <- list(chain1=0.05, chain2=0.95)


# Run the model
results <- run.jags(ap_model, n.chains=1, 
                    burnin=5000, sample=10000)

# View results

# Plot results
plot(results)
# Print results
summary(results)

# Convert the results to 'mcmc.list' object
mcmc_ap <- as.mcmc.list(results)

# Convert 'mcmc.list' to data frame for ggplot
df_ap <- as.data.frame(as.mcmc(mcmc_ap))

library(ggplot2)
library(tidyr)  # For pivot_longer

# Assuming df_ap contains the columns for ap1 and ap2
# If your data frame isn't set up this way, adjust the data extraction part accordingly.

# Convert the dataframe to long format
df_long <- pivot_longer(df_ap, cols = c(ap1, ap2), names_to = "parameter", values_to = "value")

library(ggplot2)

# Assuming df_long is already created and contains the parameters 'ap1' and 'ap2' in long format

# Generate a sample from a beta distribution
beta_data <- data.frame(value = rbeta(10000, 1, 10000))
beta_data$parameter <- "Beta Distribution"

# Combine this with your existing data
df_long <- rbind(df_long, beta_data)

# Plotting
p <- ggplot(df_long, aes(x = value, fill = parameter)) +
  geom_density(alpha = 0.6, adjust = 1) +  # Adjust can be tweaked for smoother curves
  scale_fill_manual(values = c(ap1 = "blue", ap2 = "red", `Beta Distribution` = "green")) +
  labs(title = "Overlay of Density Plots for Apparent Prevalences ap1, ap2, and Beta Distribution",
       x = "Prevalence Estimate",
       y = "Density") +
  theme_minimal()

# Print the plot
print(p)


```

# Exercise 1 - Practical

  * What if you tested 4000 individuals (n=4000) and 1200 tested positive (y=1200)?
  * What if you have prior information on ap?

# Exercise 2: Bayesian true prevalence (tp) estimation

Assuming the absence of a perfect test we do not know how many individuals are truly positive/negative.

Instead we know that n individuals are tested with an imperfect test and y have a positive result.


 * Apparent/True prevalence: ap/tp 
 * Sensitivity: Se 
 * Specificity: Sp

 * ap = P(T+) = P(T + & D+) + P(T+ &  D-) = P(D+) * P(T+|D+) + P(D-) * P(T+|D-) =>
 * ap = tp * Se + (1 - tp) * (1 - Sp)

# Create a JAGS model for true prevalence estimation

 * Parameters of interest - tp, Se, Sp 

Prior distributions

 * tp ~ dbeta(1,1) # Uniform (non-informative) prior distribution 
 * Se ~ dbeta(25.4, 3.4) # 0.85 (0.7 - 0.95)
 * Sp ~ dbeta(95, 5) # 0.77 (0.49 - 0.96)

  Data: n tested, y positive

  * Likelihood: y ~ Binomial(n,ap), 
  * ap = tp * Se + (1 - tp) * (1 - Sp)

# Write JAGS model
```{r}
tp_model <- 
  'model {
  y ~ dbin(ap,n)
  ap <- tp*Se + (1-tp)*(1-Sp)
  
  # Uniform (non-informative) prior distribution 
  tp ~ dbeta(1,1)
  # Informative priors for Se and Sp
  Se ~ dbeta(25.4, 3.4)
  Sp ~ dbeta(95, 5)
  
  #data# n, y
  #monitor# tp, Se, Sp
  #inits# tp, Se, Sp
  }
  '
```

# Let's run our JAGS model
```{r}
# Call JAGS
library(runjags)
# Provide Data 
n = 4072
y = 1210
# Initial values for pars of interest
tp <- list(chain1=0.05, chain2=0.95)
Se <- list(chain1=0.05, chain2=0.95)
Sp <- list(chain1=0.05, chain2=0.95)

# Run the model
results <- run.jags(tp_model, n.chains=2, 
                    burnin=5000, sample=10000)
## View results
# Plot results
plot(results) # Click backwards to view all plots
# Print results
summary(results)
```


# Exercise 2 - Practical

  * What if you had no prior information at all on Se,Sp?
  * What if the sample size is n=40, y=12?



# Ok for today? - Any questions?

# Sensitivity - Specificity estimation without a gold standard

  - Hui-Walter paradigm/model (1980) 
  - A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard
  - Not originally/necessarily Bayesian - implemented using Maximum Likelihood 

  - Evaluating an imperfect test against another imperfect test
  - If we don't know the true disease status, how can we estimate sensitivity or specificity for either test?

# Hui-Walter explained

https://www.youtube.com/watch?v=z6devQmW2xE&ab_channel=PolychronisKostoulas#

# We will use the data/observations from the manuscript published back in 1980.

Hui-Walter (1980) dataset Table 1

```{r}
pop_1 = matrix(nrow=3,ncol=3)
rownames(pop_1) = c("Mantoux_Test_Pos", "Mantoux_Test_Neg", "Total")
colnames(pop_1) = c("Tine_Test_Pos", "Tine_Test_Neg", "Total")

pop_1[1,1] = 14
pop_1[1,2] = 4
pop_1[2,1] = 9
pop_1[2,2] = 528
#Total rows and columns
pop_1[1,3] = pop_1[1,1] + pop_1[1,2]
pop_1[2,3] = pop_1[2,1] + pop_1[2,2]
pop_1[3,1] = pop_1[1,1] + pop_1[2,1]
pop_1[3,2] = pop_1[1,2] + pop_1[2,2]
N_1 = sum(pop_1[1,1] + pop_1[1,2] + pop_1[2,1] + pop_1[2,2])
pop_1[3,3] = N_1
pop_1
```

```{r}
## Now let's do pop_2
pop_2 = matrix(nrow=3,ncol=3)
rownames(pop_2) = c("Mantoux_Test_Pos", "Mantoux_Test_Neg", "Total")
colnames(pop_2) = c("Tine_Test_Pos", "Tine_Test_Neg", "Total")

pop_2[1,1] = 887
pop_2[1,2] = 31
pop_2[2,1] = 37
pop_2[2,2] = 367
#Total rows and columns
pop_2[1,3] = pop_2[1,1] + pop_2[1,2]
pop_2[2,3] = pop_2[2,1] + pop_2[2,2]
pop_2[3,1] = pop_2[1,1] + pop_2[2,1]
pop_2[3,2] = pop_2[1,2] + pop_2[2,2]
N_2 = sum(pop_2[1,1] + pop_2[1,2] + pop_2[2,1] + pop_2[2,2])
pop_2[3,3] = N_2
pop_2
```

# Hui-Walter model

  - A particular model formulation that was originally designed for evaluating diagnostic tests in the absence of a gold standard

  - Also known as the two_test - two_population setting/paradigm

# Model Specification ('hw_definition')

```{r}
hw_definition <- c("model{
  Population_1 ~ dmulti(prob_1, N_1)
  Population_2 ~ dmulti(prob_2, N_2)
  
  #Population_1
  
  # Test1+ Test2+
	prob_1[1] <- (prev[1] * ((se[1])*(se[2]))) + ((1-prev[1]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_1[2] <- (prev[1] * ((se[1])*(1-se[2]))) + ((1-prev[1]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_1[3] <- (prev[1] * ((1-se[1])*(se[2]))) + ((1-prev[1]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_1[4] <- (prev[1] * ((1-se[1])*(1-se[2]))) + ((1-prev[1]) * ((sp[1])*(sp[2])))
	
	#Population_2
  
  # Test1+ Test2+
	prob_2[1] <- (prev[2] * ((se[1])*(se[2]))) + ((1-prev[2]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_2[2] <- (prev[2] * ((se[1])*(1-se[2]))) + ((1-prev[2]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_2[3] <- (prev[2] * ((1-se[1])*(se[2]))) + ((1-prev[2]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_2[4] <- (prev[2] * ((1-se[1])*(1-se[2]))) + ((1-prev[2]) * ((sp[1])*(sp[2])))

  prev[1] ~ dbeta(1, 1)
  prev[2] ~ dbeta(1, 1)
  
  se[1] ~ dbeta(1, 1)I(1-sp[1], )
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)I(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Population_1, Population_2, N_1, N_2
  #monitor# prev, prob_1, prob_2, se, sp
  #inits# prev, se, sp
  }
  ")
```

# Let's run the model

```{r}
library('runjags')

Population_1 <- as.numeric(pop_1[1:2,1:2])
Population_2 <- as.numeric(pop_2[1:2,1:2])


prev <- list(chain1=c(0.05,0.99), chain2=c(0.95,0.05))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results <- run.jags(hw_definition, n.chains=2)
```

# Remember to check convergence and effective sample size!

```{r}
pt <- plot(results)
pt$`prev[1].plot1`
pt$`prev[1].plot3`

print(pt[["prev[1].plot1"]])

print(pt[["se[1].plot1"]])
print(pt[["sp[1].plot1"]])
print(pt[["sp[1].plot3"]])

summary(results)
```

# Exercise 1 

  * Run the `hw_definition` model under the following different scenarios and interpret the results in each case.

  * Change the priors for *Se[1]* and *Sp[1]* and try Beta(5,1).

# Solution - Model Specification ('hw_definition')

```{r}
hw_definition_1 <- c("model{
  Population_1 ~ dmulti(prob_1, N_1)
  Population_2 ~ dmulti(prob_2, N_2)
  
  #Population_1
  
  # Test1+ Test2+
	prob_1[1] <- (prev[1] * ((se[1])*(se[2]))) + ((1-prev[1]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_1[2] <- (prev[1] * ((se[1])*(1-se[2]))) + ((1-prev[1]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_1[3] <- (prev[1] * ((1-se[1])*(se[2]))) + ((1-prev[1]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_1[4] <- (prev[1] * ((1-se[1])*(1-se[2]))) + ((1-prev[1]) * ((sp[1])*(sp[2])))
	
	#Population_2
  
  # Test1+ Test2+
	prob_2[1] <- (prev[2] * ((se[1])*(se[2]))) + ((1-prev[2]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_2[2] <- (prev[2] * ((se[1])*(1-se[2]))) + ((1-prev[2]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_2[3] <- (prev[2] * ((1-se[1])*(se[2]))) + ((1-prev[2]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_2[4] <- (prev[2] * ((1-se[1])*(1-se[2]))) + ((1-prev[2]) * ((sp[1])*(sp[2])))

  prev[1] ~ dbeta(1, 1)
  prev[2] ~ dbeta(1, 1)
  
  se[1] ~ dbeta(5, 1)I(1-sp[1], )
  sp[1] ~ dbeta(5, 1)
  se[2] ~ dbeta(1, 1)I(1-sp[2], )
  sp[2] ~ dbeta(1, 1)

  #data# Population_1, Population_2, N_1, N_2
  #monitor# prev, prob_1, prob_2, se, sp
  #inits# prev, se, sp
  }
  ")

Population_1 <- as.numeric(pop_1[1:2,1:2])
Population_2 <- as.numeric(pop_2[1:2,1:2])


prev <- list(chain1=c(0.05,0.99), chain2=c(0.95,0.05))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results_1 <- run.jags(hw_definition_1, n.chains=2)
# Remember to check convergence and effective sample size!
plot(results_1)  # Click backwards to view all plots

pt_1 <- plot(results_1)
pt_1$`prev[1].plot1`
pt_1$`prev[1].plot3`

print(pt_1[["prev[1].plot1"]])

print(pt_1[["se[1].plot1"]])
print(pt_1[["sp[1].plot1"]])
print(pt_1[["sp[1].plot3"]])

summary(results_1)
```

# Exercise 2 

  * Remove the `I(1-sp[1], )` and `I(1-sp[2],)` from the model and run it again. What happens now?

# Solution Model Specification ('hw_definition')

```{r}
hw_definition_2 <- c("model{
  Population_1 ~ dmulti(prob_1, N_1)
  Population_2 ~ dmulti(prob_2, N_2)
  
  #Population_1
  
  # Test1+ Test2+
	prob_1[1] <- (prev[1] * ((se[1])*(se[2]))) + ((1-prev[1]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_1[2] <- (prev[1] * ((se[1])*(1-se[2]))) + ((1-prev[1]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_1[3] <- (prev[1] * ((1-se[1])*(se[2]))) + ((1-prev[1]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_1[4] <- (prev[1] * ((1-se[1])*(1-se[2]))) + ((1-prev[1]) * ((sp[1])*(sp[2])))
	
	#Population_2
  
  # Test1+ Test2+
	prob_2[1] <- (prev[2] * ((se[1])*(se[2]))) + ((1-prev[2]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_2[2] <- (prev[2] * ((se[1])*(1-se[2]))) + ((1-prev[2]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_2[3] <- (prev[2] * ((1-se[1])*(se[2]))) + ((1-prev[2]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_2[4] <- (prev[2] * ((1-se[1])*(1-se[2]))) + ((1-prev[2]) * ((sp[1])*(sp[2])))

  prev[1] ~ dbeta(1, 1)
  prev[2] ~ dbeta(1, 1)
  
  se[1] ~ dbeta(1, 1)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)
  sp[2] ~ dbeta(1, 1)

  #data# Population_1, Population_2, N_1, N_2
  #monitor# prev, prob_1, prob_2, se, sp
  #inits# prev, se, sp
  }
  ")

Population_1 <- as.numeric(pop_1[1:2,1:2])
Population_2 <- as.numeric(pop_2[1:2,1:2])


prev <- list(chain1=c(0.05,0.99), chain2=c(0.95,0.05))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results_2 <- run.jags(hw_definition_2, n.chains=2)
# Remember to check convergence and effective sample size!
plot(results_2)  # Click backwards to view all plots

pt_2 <- plot(results_2)
pt_2$`prev[1].plot1`
pt_2$`prev[1].plot3`

print(pt_2[["prev[1].plot1"]])

print(pt_2[["se[1].plot1"]])
print(pt_2[["sp[1].plot1"]])
print(pt_2[["sp[1].plot3"]])

print(pt_2[["se[2].plot1"]])

summary(results_2)
```

# Exercise 3

Try to run the model with different initial values, that explore the whole parameter space and removing `I(1-sp[1],)`.

For example try it with:
```{r}
se <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
sp <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
```

# Solution

The model is the same like as in the above scenario (hw_definition_2)

```{r}
Population_1 <- as.numeric(pop_1[1:2,1:2])
Population_2 <- as.numeric(pop_2[1:2,1:2])


prev <- list(chain1=c(0.05,0.99), chain2=c(0.95,0.05))
se <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
sp <- list(chain1=c(0.01,0.99), chain2=c(0.99,0.01))
```

# 

```{r}
results_3 <- run.jags(hw_definition_2, n.chains=2, sample = 100000)
# Remember to check convergence and effective sample size!
plot(results_3)  # Click backwards to view all plots

pt_3 <- plot(results_3)
pt_3$`prev[1].plot1`
pt_3$`prev[1].plot3`

print(pt_3[["prev[1].plot1"]])

print(pt_3[["se[1].plot1"]])
print(pt_3[["sp[1].plot1"]])
print(pt_3[["sp[1].plot3"]])
print(pt_3[["se[2].plot1"]])

summary(results_3)
```

  - The model fails to converge to one solution, but reports two solution that are complementary

# Exercise 4

Run the model with only 1 population (either pop_1 or pop_2). What happens then?

  * We'll remove population 2


# Solution
Model Specification ('hw_definition')

```{r}
hw_definition_4 <- c("model{
  Population_1 ~ dmulti(prob_1, N_1)

  #Population_1
  
  # Test1+ Test2+
	prob_1[1] <- (prev[1] * ((se[1])*(se[2]))) + ((1-prev[1]) * ((1-sp[1])*(1-sp[2])))
  
  # Test1+ Test2-
	prob_1[2] <- (prev[1] * ((se[1])*(1-se[2]))) + ((1-prev[1]) * ((1-sp[1])*(sp[2])))

  # Test1- Test2+
	prob_1[3] <- (prev[1] * ((1-se[1])*(se[2]))) + ((1-prev[1]) * ((sp[1])*(1-sp[2])))

  # Test1- Test2-
	prob_1[4] <- (prev[1] * ((1-se[1])*(1-se[2]))) + ((1-prev[1]) * ((sp[1])*(sp[2])))
	

  prev[1] ~ dbeta(1, 1)
  se[1] ~ dbeta(1, 1)I(1-sp[1],)
  sp[1] ~ dbeta(1, 1)
  se[2] ~ dbeta(1, 1)I(1-sp[2],)
  sp[2] ~ dbeta(1, 1)

  #data# Population_1, N_1
  #monitor# prev, prob_1,  se, sp
  #inits# prev, se, sp
  }
  ")

Population_1 <- as.numeric(pop_1[1:2,1:2])


prev <- list(chain1=c(0.05), chain2=c(0.95))
se <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))
sp <- list(chain1=c(0.5,0.99), chain2=c(0.99,0.5))

results_4 <- run.jags(hw_definition_4, n.chains=2)
# Remember to check convergence and effective sample size!
plot(results_4)  # Click backwards to view all plots

pt_4 <- plot(results_4)
pt_4$`prev.plot1`
pt_4$`prev.plot3`

print(pt_4[["prev.plot1"]])

print(pt_4[["se[1].plot1"]])
print(pt_4[["sp[1].plot1"]])
print(pt_4[["sp[1].plot3"]])
print(pt_4[["se[2].plot3"]])
print(pt_4[["se[2].plot4"]])
print(pt_4[["se[2].plot5"]])

summary(results_4)
```

How do the results look? # Compare the results with the scenarios above?  Try also by removing population 1

